import torch
import time
import threading
import subprocess
import signal
import sys
import math
import os
from datetime import datetime

class GPUPrimeFinder:
    def __init__(self, target_usage=3.0, max_memory_gb=2.0):
        """
        GPUë¡œ ì†Œìˆ˜ ì°¾ê¸° - ì‚¬ìš©ë¥ ê³¼ ë©”ëª¨ë¦¬ ì œí•œ
        
        Args:
            target_usage: ëª©í‘œ GPU ì‚¬ìš©ë¥  (%)
            max_memory_gb: ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB)
        """
        self.target_usage = target_usage
        self.max_memory_bytes = int(max_memory_gb * 1024**3)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.running = False
        
        # ì†Œìˆ˜ ì°¾ê¸° ìƒíƒœ
        self.current_range_start = 1000000  # 100ë§Œë¶€í„° ì‹œì‘
        self.batch_size = 50000  # í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ«ì ê°œìˆ˜
        self.found_primes = []
        self.total_checked = 0
        self.primes_found = 0
        self.session_start_time = datetime.now()
        
        # ì„±ëŠ¥ ì¡°ì ˆ
        self.work_intensity = 15  # ì´ˆê¸°ê°’
        self.rest_time = 0.1  # ë°°ì¹˜ ê°„ íœ´ì‹ ì‹œê°„
        
        # íŒŒì¼ ê´€ë¦¬
        self.results_file = "gpu_primes_collection.txt"
        self.load_previous_progress()
        
        print(f"ğŸ”¢ GPU ì†Œìˆ˜ ì°¾ê¸° ì‹œì‘")
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"ì‚¬ìš© ì œí•œ: {max_memory_gb} GB")
        print(f"ëª©í‘œ ì‚¬ìš©ë¥ : {target_usage}%")
        print(f"ì‹œì‘ ë²”ìœ„: {self.current_range_start:,}")
        print(f"ê²°ê³¼ íŒŒì¼: {self.results_file}")
        print("-" * 50)
        
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
    
    def signal_handler(self, signum, frame):
        print(f"\nì¢…ë£Œ ì¤‘...")
        self.save_final_summary()
        self.stop()
        sys.exit(0)
    
    def get_gpu_usage(self):
        """GPU ì‚¬ìš©ë¥  í™•ì¸"""
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                return float(result.stdout.strip())
        except:
            pass
        return 0.0
    
    def get_gpu_memory_usage(self):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (bytes)"""
        try:
            return torch.cuda.memory_allocated(self.device)
        except:
            return 0
    
    def load_previous_progress(self):
        """ì´ì „ ì§„í–‰ ìƒí™© ë¡œë“œ"""
        try:
            if os.path.exists(self.results_file):
                with open(self.results_file, 'r') as f:
                    lines = f.readlines()
                
                # ë§ˆì§€ë§‰ ë²”ìœ„ ì°¾ê¸°
                last_range = 0
                total_primes = 0
                
                for line in lines:
                    if line.startswith("# í˜„ì¬ ë²”ìœ„:"):
                        try:
                            last_range = int(line.split(":")[1].strip().replace(",", ""))
                        except:
                            pass
                    elif line.startswith("# ì´ ì†Œìˆ˜:"):
                        try:
                            total_primes = int(line.split(":")[1].strip().replace(",", ""))
                        except:
                            pass
                
                if last_range > self.current_range_start:
                    self.current_range_start = last_range
                    print(f"ğŸ“‚ ì´ì „ ì§„í–‰ ìƒí™© ë¡œë“œ: {last_range:,}ë¶€í„° ê³„ì†")
                    print(f"ğŸ“Š ì´ì „ê¹Œì§€ ë°œê²¬í•œ ì†Œìˆ˜: {total_primes:,}ê°œ")
                
        except Exception as e:
            print(f"ì´ì „ ì§„í–‰ ìƒí™© ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def save_primes_to_file(self, new_primes):
        """ìƒˆë¡œ ë°œê²¬í•œ ì†Œìˆ˜ë“¤ì„ íŒŒì¼ì— ì¶”ê°€"""
        if not new_primes:
            return
        
        try:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” ìƒì„±
            if not os.path.exists(self.results_file):
                with open(self.results_file, 'w') as f:
                    f.write("# GPU ì†Œìˆ˜ ì»¬ë ‰ì…˜ ğŸ“Š\n")
                    f.write(f"# GPU: {torch.cuda.get_device_name()}\n")
                    f.write(f"# ì‹œì‘ì¼: {self.session_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write("# ----------------------------------------\n")
                    f.write("# í˜•ì‹: ì†Œìˆ˜ (ë°œê²¬ì‹œê°„)\n")
                    f.write("# ----------------------------------------\n\n")
            
            # ìƒˆ ì†Œìˆ˜ë“¤ ì¶”ê°€
            with open(self.results_file, 'a') as f:
                current_time = datetime.now().strftime('%H:%M:%S')
                
                for prime in new_primes:
                    f.write(f"{prime} ({current_time})\n")
                
                # í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                f.write(f"\n# ì—…ë°ì´íŠ¸: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# í˜„ì¬ ë²”ìœ„: {self.current_range_start:,}\n")
                f.write(f"# ì´ë²ˆ ì„¸ì…˜ ë°œê²¬: {self.primes_found:,}ê°œ\n")
                f.write(f"# ì´ í™•ì¸í•œ ìˆ˜: {self.total_checked:,}ê°œ\n")
                
                # ì„±ëŠ¥ í†µê³„
                elapsed = (datetime.now() - self.session_start_time).total_seconds()
                if elapsed > 0:
                    rate = self.total_checked / elapsed
                    f.write(f"# í™•ì¸ ì†ë„: {rate:.0f} ìˆ˜/ì´ˆ\n")
                
                f.write("# ----------------------------------------\n\n")
                f.flush()
            
            print(f"ğŸ’¾ {len(new_primes)}ê°œ ì†Œìˆ˜ íŒŒì¼ì— ì €ì¥ë¨")
            
        except Exception as e:
            print(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def save_final_summary(self):
        """ìµœì¢… ìš”ì•½ ì €ì¥"""
        try:
            with open(self.results_file, 'a') as f:
                f.write(f"\n{'='*50}\n")
                f.write(f"# ì„¸ì…˜ ì¢…ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                
                elapsed = (datetime.now() - self.session_start_time).total_seconds()
                hours = elapsed // 3600
                minutes = (elapsed % 3600) // 60
                seconds = elapsed % 60
                
                f.write(f"# ì‹¤í–‰ ì‹œê°„: {int(hours)}ì‹œê°„ {int(minutes)}ë¶„ {int(seconds)}ì´ˆ\n")
                f.write(f"# ìµœì¢… ë²”ìœ„: {self.current_range_start:,}\n")
                f.write(f"# ì´ë²ˆ ì„¸ì…˜ ê²°ê³¼:\n")
                f.write(f"#   - í™•ì¸í•œ ìˆ˜: {self.total_checked:,}ê°œ\n")
                f.write(f"#   - ë°œê²¬í•œ ì†Œìˆ˜: {self.primes_found:,}ê°œ\n")
                
                if self.total_checked > 0:
                    prime_ratio = (self.primes_found / self.total_checked) * 100
                    f.write(f"#   - ì†Œìˆ˜ ë¹„ìœ¨: {prime_ratio:.4f}%\n")
                
                if elapsed > 0:
                    rate = self.total_checked / elapsed
                    f.write(f"#   - í‰ê·  í™•ì¸ ì†ë„: {rate:.0f} ìˆ˜/ì´ˆ\n")
                
                f.write(f"{'='*50}\n\n")
                f.flush()
            
            print(f"ğŸ“‹ ìµœì¢… ìš”ì•½ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ìµœì¢… ìš”ì•½ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def is_prime_gpu_batch(self, numbers):
        """GPUì—ì„œ ë°°ì¹˜ë¡œ ì†Œìˆ˜ íŒë³„"""
        try:
            # ì…ë ¥ì„ GPU í…ì„œë¡œ ë³€í™˜
            nums = torch.tensor(numbers, device=self.device, dtype=torch.long)
            batch_size = len(numbers)
            
            # ê²°ê³¼ í…ì„œ (True = ì†Œìˆ˜)
            is_prime = torch.ones(batch_size, device=self.device, dtype=torch.bool)
            
            # 1ê³¼ ì§ìˆ˜ ì œê±° (2 ì œì™¸)
            is_prime = is_prime & (nums > 1)
            is_prime = is_prime & ((nums == 2) | (nums % 2 != 0))
            
            # 3ë¶€í„° sqrt(n)ê¹Œì§€ì˜ í™€ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ í™•ì¸
            max_num = int(math.sqrt(max(numbers))) + 1
            for divisor in range(3, max_num, 2):
                if not self.running:  # ì¤‘ë‹¨ ì²´í¬
                    break
                
                # GPUì—ì„œ ë‚˜ë¨¸ì§€ ì—°ì‚°
                divisor_tensor = torch.tensor(divisor, device=self.device)
                remainder = nums % divisor_tensor
                is_prime = is_prime & (remainder != 0)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                if self.get_gpu_memory_usage() > self.max_memory_bytes:
                    print("âš ï¸ ë©”ëª¨ë¦¬ í•œë„ ì´ˆê³¼, ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ")
                    break
            
            # CPUë¡œ ê²°ê³¼ ë³µì‚¬
            result = is_prime.cpu().numpy()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            del nums, is_prime
            torch.cuda.empty_cache()
            
            return result
            
        except Exception as e:
            print(f"GPU ì†Œìˆ˜ íŒë³„ ì˜¤ë¥˜: {e}")
            torch.cuda.empty_cache()
            return [False] * len(numbers)
    
    def prime_finding_worker(self):
        """ì†Œìˆ˜ ì°¾ê¸° ë©”ì¸ ì›Œì»¤"""
        print("ğŸ” ì†Œìˆ˜ ì°¾ê¸° ì‹œì‘...")
        
        while self.running:
            try:
                # í˜„ì¬ ë²”ìœ„ ì„¤ì •
                range_end = self.current_range_start + self.batch_size
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
                current_memory = self.get_gpu_memory_usage()
                if current_memory > self.max_memory_bytes * 0.8:  # 80% ì´ˆê³¼ì‹œ
                    self.batch_size = max(1000, self.batch_size // 2)
                    print(f"ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ: {self.batch_size}")
                
                # ìˆ«ì ë°°ì¹˜ ìƒì„± (í™€ìˆ˜ë§Œ, ì„±ëŠ¥ ìµœì í™”)
                if self.current_range_start % 2 == 0:
                    self.current_range_start += 1
                
                numbers = list(range(self.current_range_start, range_end, 2))
                
                # ê°•ë„ ì¡°ì ˆ: ì¼ë¶€ë§Œ ì²˜ë¦¬
                actual_batch_size = max(100, len(numbers) * self.work_intensity // 30)
                numbers = numbers[:actual_batch_size]
                
                # GPUì—ì„œ ì†Œìˆ˜ íŒë³„
                prime_flags = self.is_prime_gpu_batch(numbers)
                
                # ì†Œìˆ˜ ì¶”ì¶œ ë° ì¦‰ì‹œ ì €ì¥
                new_primes = [num for num, is_prime in zip(numbers, prime_flags) if is_prime]
                
                # íŒŒì¼ì— ì¦‰ì‹œ ì €ì¥ (í° ì†Œìˆ˜ë“¤ì€ ë°”ë¡œ ì €ì¥)
                if new_primes:
                    self.save_primes_to_file(new_primes)
                
                # ê²°ê³¼ ì—…ë°ì´íŠ¸
                self.found_primes.extend(new_primes)
                self.total_checked += len(numbers)
                self.primes_found += len(new_primes)
                
                # ìµœê·¼ ì†Œìˆ˜ ì¶œë ¥
                if new_primes:
                    recent_primes = new_primes[-min(3, len(new_primes)):]
                    print(f"âœ¨ ìƒˆ ì†Œìˆ˜ ë°œê²¬: {recent_primes}")
                
                # ë²”ìœ„ ì—…ë°ì´íŠ¸
                self.current_range_start = range_end
                
                # ê°•ë„ì— ë”°ë¥¸ íœ´ì‹
                time.sleep(self.rest_time)
                
            except Exception as e:
                print(f"ì†Œìˆ˜ ì°¾ê¸° ì˜¤ë¥˜: {e}")
                torch.cuda.empty_cache()
                time.sleep(1)
        
        print("ğŸ” ì†Œìˆ˜ ì°¾ê¸° ì¢…ë£Œ")
    
    def monitor_and_adjust(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì¡°ì •"""
        print("ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        while self.running:
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            gpu_usage = self.get_gpu_usage()
            memory_usage = self.get_gpu_memory_usage()
            memory_gb = memory_usage / (1024**3)
            
            # í†µê³„ ì¶œë ¥
            print(f"ğŸ“ˆ GPU: {gpu_usage:.1f}% | ë©”ëª¨ë¦¬: {memory_gb:.2f}GB | ê°•ë„: {self.work_intensity}")
            print(f"ğŸ”¢ ì´ë²ˆ ì„¸ì…˜: {self.total_checked:,}ê°œ í™•ì¸, {self.primes_found:,}ê°œ ì†Œìˆ˜ ë°œê²¬")
            if self.primes_found > 0:
                print(f"ğŸ¯ í˜„ì¬ ë²”ìœ„: {self.current_range_start:,} | ìµœê·¼ ì†Œìˆ˜: {self.found_primes[-1]:,}")
                
            # ì‹¤í–‰ ì‹œê°„ ë° ì†ë„ ê³„ì‚°
            elapsed = (datetime.now() - self.session_start_time).total_seconds()
            if elapsed > 0:
                rate = self.total_checked / elapsed
                print(f"âš¡ ì‹¤í–‰ì‹œê°„: {elapsed/60:.1f}ë¶„ | í™•ì¸ì†ë„: {rate:.0f}ìˆ˜/ì´ˆ")
            
            # ì‚¬ìš©ë¥  ì¡°ì •
            if gpu_usage < self.target_usage * 0.8:
                if self.work_intensity < 30:
                    self.work_intensity += 1
                    self.rest_time = max(0.01, self.rest_time * 0.9)
                    print(f"â†’ ê°•ë„ ì¦ê°€: {self.work_intensity}")
            elif gpu_usage > self.target_usage * 1.3:
                if self.work_intensity > 5:
                    self.work_intensity -= 1
                    self.rest_time = min(0.5, self.rest_time * 1.1)
                    print(f"â†’ ê°•ë„ ê°ì†Œ: {self.work_intensity}")
            else:
                print("âœ… ëª©í‘œ ì‚¬ìš©ë¥  ë‹¬ì„±")
            
            # ë©”ëª¨ë¦¬ ì´ˆê³¼ ì‹œ ê°•ì œ ì •ë¦¬
            # if memory_gb > self.max_memory_gb * 0.9:
            if memory_gb > (self.max_memory_bytes / (1024**3)) * 0.9:
                print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
                torch.cuda.empty_cache()
            
            print("-" * 50)
            time.sleep(10)
    
    def start(self):
        """ì†Œìˆ˜ ì°¾ê¸° ì‹œì‘"""
        if not torch.cuda.is_available():
            print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        print("ğŸš€ GPU ì†Œìˆ˜ ì°¾ê¸° ì‹œì‘...")
        self.running = True
        
        # ì†Œìˆ˜ ì°¾ê¸° ì›Œì»¤ ì‹œì‘
        prime_worker = threading.Thread(target=self.prime_finding_worker)
        prime_worker.daemon = True
        prime_worker.start()
        
        # ëª¨ë‹ˆí„°ë§ ì›Œì»¤ ì‹œì‘
        monitor_worker = threading.Thread(target=self.monitor_and_adjust)
        monitor_worker.daemon = True
        monitor_worker.start()
        
        print("ğŸ’¡ Ctrl+Cë¡œ ì¢…ë£Œ ë° ê²°ê³¼ ì €ì¥")
        print("=" * 50)
        
        try:
            while self.running:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nì‚¬ìš©ì ì¢…ë£Œ ìš”ì²­")
            self.save_final_summary()
            self.stop()
    
    def stop(self):
        """ì¢…ë£Œ"""
        print("ğŸ›‘ ì†Œìˆ˜ ì°¾ê¸° ì¢…ë£Œ ì¤‘...")
        self.running = False
        torch.cuda.empty_cache()
        print("âœ… ì™„ë£Œ")

def main():
    # GPU ì‚¬ìš©ë¥  3%, ë©”ëª¨ë¦¬ 2GB ì œí•œ
    finder = GPUPrimeFinder(target_usage=3.0, max_memory_gb=2.0)
    finder.start()

if __name__ == "__main__":
    main()

