by claude, Sonnet 4.5
================================================================================
GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: NVIDIA H100 80GB HBM3
================================================================================

[1/5] ì—°ì‚° ì„±ëŠ¥ ì¸¡ì • ì¤‘... (í–‰ë ¬ í¬ê¸°: 8192x8192)
  â†’ ì¸¡ì •ê°’: 724.18 TFLOPS
  â†’ ìˆ˜ì‹: (2 Ã— 8192Â³ Ã— 100) / 0.1518ì´ˆ / 10Â¹Â²

[2/5] ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì¸¡ì • ì¤‘... (ë°ì´í„° í¬ê¸°: 2.0GB)
  â†’ ì¸¡ì •ê°’: 3038.91 GB/s
  â†’ ìˆ˜ì‹: (2.0GB Ã— 50 Ã— 2) / 0.0707ì´ˆ

[3/5] ì´ë¯¸ì§€ ìƒì„± ì²˜ë¦¬ëŸ‰ ì¸¡ì • ì¤‘... (FP32, BS=32, 512x512)
  â†’ ì¸¡ì •ê°’: 318.88 images/sec
  â†’ ìˆ˜ì‹: (32 Ã— 100) / 10.0350ì´ˆ

[3/5] ì´ë¯¸ì§€ ìƒì„± ì²˜ë¦¬ëŸ‰ ì¸¡ì • ì¤‘... (FP16, BS=32, 512x512)
  â†’ ì¸¡ì •ê°’: 552.69 images/sec
  â†’ ìˆ˜ì‹: (32 Ã— 100) / 5.7899ì´ˆ

[4/5] ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚° ì¤‘...
  â†’ ì—°ì‚° ì ìˆ˜: 73.2/100
    ìˆ˜ì‹: (724.18 / 989) Ã— 100
  â†’ ë©”ëª¨ë¦¬ ì ìˆ˜: 90.7/100
    ìˆ˜ì‹: (3038.91 / 3350) Ã— 100
  â†’ ì²˜ë¦¬ëŸ‰ ì ìˆ˜: 69.1/100
    ìˆ˜ì‹: (552.69 / 800) Ã— 100

[5/5] ìµœì¢… ì¢…í•© ì ìˆ˜ ê³„ì‚° ì¤‘...
  â†’ ìˆ˜ì‹: (73.2 Ã— 0.4) + (90.7 Ã— 0.3) + (69.1 Ã— 0.3)
  â†’ ê³„ì‚°: 29.3 + 27.2 + 20.7 = 77.2

================================================================================
ğŸ“Š GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìµœì¢… ë³´ê³ ì„œ
================================================================================

ğŸ–¥ï¸  GPU ëª¨ë¸: NVIDIA H100 80GB HBM3
ğŸ’¾ ì´ ë©”ëª¨ë¦¬: 79.19 GB

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ˆ ì›ì‹œ ì„±ëŠ¥ ì¸¡ì •ê°’
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âš¡ ì—°ì‚° ì„±ëŠ¥ (FP16):          724.18 TFLOPS
  ğŸš€ ë©”ëª¨ë¦¬ ëŒ€ì—­í­:            3038.91 GB/s
  ğŸ–¼ï¸  FP32 ì´ë¯¸ì§€ ì²˜ë¦¬ëŸ‰:       318.88 images/sec
  ğŸ–¼ï¸  FP16 ì´ë¯¸ì§€ ì²˜ë¦¬ëŸ‰:       552.69 images/sec
  â±ï¸  FP16 ê°€ì† ë¹„ìœ¨:             1.73x

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ ì •ê·œí™” ì ìˆ˜ (H100 ì´ë¡  ì„±ëŠ¥ ê¸°ì¤€)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ì—°ì‚° ì„±ëŠ¥:   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  73.2/100
  ë©”ëª¨ë¦¬ ì„±ëŠ¥: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘]  90.7/100
  ì²˜ë¦¬ëŸ‰ ì„±ëŠ¥: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  69.1/100

================================================================================
ğŸ† ì¢…í•© ì„±ëŠ¥ ì ìˆ˜: 77.2/100 - A+ (ìš°ìˆ˜)
================================================================================

ğŸ’¡ ì„±ëŠ¥ í•´ì„:
  â€¢ ì´ë¡  ëŒ€ë¹„ ì‹¤ì œ ë‹¬ì„±ë¥ : ~77%
  â€¢ ì—°ì‚° íš¨ìœ¨ì„±: 73.2% (ì´ë¡  989 TFLOPS ëŒ€ë¹„)
  â€¢ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: 90.7% (ì´ë¡  3,350 GB/s ëŒ€ë¹„)

ğŸ“ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ê¶Œì¥ ì‚¬í•­:
  â€¢ ë°°ì¹˜ í¬ê¸°: 16-32 (512x512 ê¸°ì¤€)
  â€¢ ì˜ˆìƒ í•™ìŠµ ì†ë„: ì‹œê°„ë‹¹ ì•½ 1990K ì´ë¯¸ì§€
  â€¢ FP16 Mixed Precision í•„ìˆ˜ ê¶Œì¥ (1.7x ê°€ì†)
  â€¢ Gradient Checkpointing ê³ ë ¤ (ë” í° ë°°ì¹˜ ê°€ëŠ¥)

ğŸ’¾ JSON ê²°ê³¼:
{
  "device": "NVIDIA H100 80GB HBM3",
  "metrics": {
    "compute_tflops": 724.1798667473894,
    "memory_bandwidth_gbs": 3038.9071562170175,
    "fp32_throughput": 318.88241108264145,
    "fp16_throughput": 552.6872287475356,
    "memory_total_gb": 79.1895751953125
  },
  "scores": {
    "compute": 73.22344456495343,
    "memory": 90.71364645423932,
    "throughput": 69.08590359344196,
    "overall": 77.22924284028576,
    "rating": "A+ (ìš°ìˆ˜)"
  }
}


import torch
import torch.nn as nn
import time
import numpy as np
from dataclasses import dataclass
from typing import Dict, Tuple
import json

@dataclass
class GPUBenchmarkResult:
    """GPU ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    # ì›ì‹œ ì¸¡ì •ê°’
    compute_tflops: float  # ì—°ì‚° ì„±ëŠ¥ (TFLOPS)
    memory_bandwidth_gbs: float  # ë©”ëª¨ë¦¬ ëŒ€ì—­í­ (GB/s)
    fp16_throughput: float  # FP16 ì´ë¯¸ì§€ ì²˜ë¦¬ëŸ‰ (images/sec)
    fp32_throughput: float  # FP32 ì´ë¯¸ì§€ ì²˜ë¦¬ëŸ‰ (images/sec)
    memory_total_gb: float  # ì´ ë©”ëª¨ë¦¬ (GB)
    
    # ì •ê·œí™”ëœ ì ìˆ˜ (0-100)
    compute_score: float
    memory_score: float
    throughput_score: float
    
    # ìµœì¢… ì¢…í•© ì ìˆ˜
    overall_score: float
    performance_rating: str

class GPUPerformanceBenchmark:
    """GPU ì„±ëŠ¥ì„ ì¢…í•© í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    # ì°¸ì¡° ì„±ëŠ¥ (H100 ì´ë¡ ê°’ ê¸°ì¤€)
    H100_REFERENCE = {
        'compute_tflops': 989.0,  # FP16 Tensor Core ì´ë¡  ì„±ëŠ¥
        'memory_bandwidth': 3350.0,  # HBM3 ì´ë¡  ëŒ€ì—­í­
        'fp16_throughput': 800.0,  # ì˜ˆìƒ ì´ë¯¸ì§€ ì²˜ë¦¬ëŸ‰ (512x512, BS=32)
    }
    
    A100_REFERENCE = {
        'compute_tflops': 312.0,  # FP16 Tensor Core ì´ë¡  ì„±ëŠ¥
        'memory_bandwidth': 2039.0,  # HBM2e ì´ë¡  ëŒ€ì—­í­
        'fp16_throughput': 400.0,  # ì˜ˆìƒ ì´ë¯¸ì§€ ì²˜ë¦¬ëŸ‰
    }
    
    def __init__(self, device_id: int = 0):
        self.device = torch.device(f'cuda:{device_id}')
        self.device_name = torch.cuda.get_device_name(device_id)
        self.device_id = device_id
        
    def measure_compute_performance(self, size: int = 8192, iterations: int = 100) -> float:
        """
        ì—°ì‚° ì„±ëŠ¥ ì¸¡ì • (TFLOPS)
        
        ê³„ì‚°ì‹:
        TFLOPS = (ì—°ì‚° íšŸìˆ˜ Ã— ë°˜ë³µ íšŸìˆ˜) / (ì‹¤í–‰ ì‹œê°„ Ã— 10^12)
        í–‰ë ¬ê³± ì—°ì‚° íšŸìˆ˜ = 2 Ã— M Ã— N Ã— K (M=N=K=size)
        """
        print(f"\n[1/5] ì—°ì‚° ì„±ëŠ¥ ì¸¡ì • ì¤‘... (í–‰ë ¬ í¬ê¸°: {size}x{size})")
        
        A = torch.randn(size, size, device=self.device, dtype=torch.float16)
        B = torch.randn(size, size, device=self.device, dtype=torch.float16)
        
        # Warmup
        for _ in range(10):
            C = torch.matmul(A, B)
        torch.cuda.synchronize()
        
        # ì‹¤ì œ ì¸¡ì •
        start = time.time()
        for _ in range(iterations):
            C = torch.matmul(A, B)
        torch.cuda.synchronize()
        elapsed = time.time() - start
        
        # FLOPS ê³„ì‚°: í–‰ë ¬ê³± ì—°ì‚°ëŸ‰ = 2 Ã— M Ã— N Ã— K
        flops_per_matmul = 2 * size * size * size
        total_flops = flops_per_matmul * iterations
        tflops = total_flops / elapsed / 1e12
        
        print(f"  â†’ ì¸¡ì •ê°’: {tflops:.2f} TFLOPS")
        print(f"  â†’ ìˆ˜ì‹: (2 Ã— {size}Â³ Ã— {iterations}) / {elapsed:.4f}ì´ˆ / 10Â¹Â²")
        
        return tflops
    
    def measure_memory_bandwidth(self, size_gb: float = 2.0, iterations: int = 50) -> float:
        """
        ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì¸¡ì • (GB/s)
        
        ê³„ì‚°ì‹:
        ëŒ€ì—­í­ = (ë°ì´í„° í¬ê¸° Ã— ë°˜ë³µ íšŸìˆ˜ Ã— 2) / ì‹¤í–‰ ì‹œê°„
        Ã— 2ëŠ” ì½ê¸°+ì“°ê¸° ë•Œë¬¸
        """
        print(f"\n[2/5] ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì¸¡ì • ì¤‘... (ë°ì´í„° í¬ê¸°: {size_gb:.1f}GB)")
        
        num_elements = int(size_gb * 1024**3 / 4)  # float32 = 4 bytes
        data = torch.randn(num_elements, device=self.device, dtype=torch.float32)
        
        # Warmup
        for _ in range(5):
            result = data * 2.0
        torch.cuda.synchronize()
        
        # ì‹¤ì œ ì¸¡ì •
        start = time.time()
        for _ in range(iterations):
            result = data * 2.0  # ì½ê¸° + ì“°ê¸°
        torch.cuda.synchronize()
        elapsed = time.time() - start
        
        # ëŒ€ì—­í­ ê³„ì‚° (ì½ê¸° + ì“°ê¸° = 2ë°°)
        bytes_transferred = num_elements * 4 * iterations * 2
        bandwidth_gbs = bytes_transferred / elapsed / 1e9
        
        print(f"  â†’ ì¸¡ì •ê°’: {bandwidth_gbs:.2f} GB/s")
        print(f"  â†’ ìˆ˜ì‹: ({size_gb:.1f}GB Ã— {iterations} Ã— 2) / {elapsed:.4f}ì´ˆ")
        
        return bandwidth_gbs
    
    def measure_image_generation_throughput(
        self, 
        batch_size: int = 32, 
        image_size: int = 512,
        use_fp16: bool = False
    ) -> float:
        """
        ì´ë¯¸ì§€ ìƒì„± ì²˜ë¦¬ëŸ‰ ì¸¡ì • (images/sec)
        
        ê³„ì‚°ì‹:
        ì²˜ë¦¬ëŸ‰ = (ë°°ì¹˜ í¬ê¸° Ã— ë°˜ë³µ íšŸìˆ˜) / ì‹¤í–‰ ì‹œê°„
        """
        dtype_str = "FP16" if use_fp16 else "FP32"
        print(f"\n[3/5] ì´ë¯¸ì§€ ìƒì„± ì²˜ë¦¬ëŸ‰ ì¸¡ì • ì¤‘... ({dtype_str}, BS={batch_size}, {image_size}x{image_size})")
        
        class SimpleUNet(nn.Module):
            def __init__(self):
                super().__init__()
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(64, 128, 3, stride=2, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(128, 256, 3, stride=2, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(256, 512, 3, stride=2, padding=1),
                    nn.ReLU(),
                )
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
                    nn.ReLU(),
                    nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
                    nn.ReLU(),
                    nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(64, 3, 3, padding=1),
                )
            
            def forward(self, x):
                x = self.encoder(x)
                x = self.decoder(x)
                return x
        
        model = SimpleUNet().to(self.device)
        optimizer = torch.optim.Adam(model.parameters())
        criterion = nn.MSELoss()
        
        if use_fp16:
            scaler = torch.amp.GradScaler('cuda')
        
        dummy_input = torch.randn(
            batch_size, 3, image_size, image_size, 
            device=self.device,
            dtype=torch.float32  # í•­ìƒ FP32ë¡œ ì…ë ¥ (autocastê°€ ìë™ ë³€í™˜)
        )
        
        # Warmup
        for _ in range(10):
            if use_fp16:
                with torch.amp.autocast('cuda', dtype=torch.float16):
                    output = model(dummy_input)
                    loss = criterion(output, dummy_input)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                output = model(dummy_input)
                loss = criterion(output, dummy_input)
                loss.backward()
                optimizer.step()
            optimizer.zero_grad()
        
        torch.cuda.synchronize()
        
        # ì‹¤ì œ ì¸¡ì •
        iterations = 100
        start = time.time()
        
        for _ in range(iterations):
            if use_fp16:
                with torch.amp.autocast('cuda', dtype=torch.float16):
                    output = model(dummy_input)
                    loss = criterion(output, dummy_input)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                output = model(dummy_input)
                loss = criterion(output, dummy_input)
                loss.backward()
                optimizer.step()
            optimizer.zero_grad()
        
        torch.cuda.synchronize()
        elapsed = time.time() - start
        
        throughput = (batch_size * iterations) / elapsed
        
        print(f"  â†’ ì¸¡ì •ê°’: {throughput:.2f} images/sec")
        print(f"  â†’ ìˆ˜ì‹: ({batch_size} Ã— {iterations}) / {elapsed:.4f}ì´ˆ")
        
        return throughput
    
    def calculate_normalized_scores(
        self,
        compute_tflops: float,
        memory_bandwidth: float,
        fp16_throughput: float
    ) -> Tuple[float, float, float]:
        """
        ê° ì§€í‘œë¥¼ 0-100 ì ìˆ˜ë¡œ ì •ê·œí™”
        
        ì ìˆ˜ ê³„ì‚°ì‹:
        ì ìˆ˜ = (ì¸¡ì •ê°’ / ì°¸ì¡°ê°’) Ã— 100
        ë‹¨, 100ì  ì´ˆê³¼ì‹œ 100ì ìœ¼ë¡œ ì œí•œ
        """
        print(f"\n[4/5] ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚° ì¤‘...")
        
        # H100 ì´ë¡  ì„±ëŠ¥ ëŒ€ë¹„ í¼ì„¼íŠ¸ë¡œ ì ìˆ˜í™”
        compute_score = min((compute_tflops / self.H100_REFERENCE['compute_tflops']) * 100, 100)
        memory_score = min((memory_bandwidth / self.H100_REFERENCE['memory_bandwidth']) * 100, 100)
        throughput_score = min((fp16_throughput / self.H100_REFERENCE['fp16_throughput']) * 100, 100)
        
        print(f"  â†’ ì—°ì‚° ì ìˆ˜: {compute_score:.1f}/100")
        print(f"    ìˆ˜ì‹: ({compute_tflops:.2f} / {self.H100_REFERENCE['compute_tflops']:.0f}) Ã— 100")
        print(f"  â†’ ë©”ëª¨ë¦¬ ì ìˆ˜: {memory_score:.1f}/100")
        print(f"    ìˆ˜ì‹: ({memory_bandwidth:.2f} / {self.H100_REFERENCE['memory_bandwidth']:.0f}) Ã— 100")
        print(f"  â†’ ì²˜ë¦¬ëŸ‰ ì ìˆ˜: {throughput_score:.1f}/100")
        print(f"    ìˆ˜ì‹: ({fp16_throughput:.2f} / {self.H100_REFERENCE['fp16_throughput']:.0f}) Ã— 100")
        
        return compute_score, memory_score, throughput_score
    
    def calculate_overall_score(
        self,
        compute_score: float,
        memory_score: float,
        throughput_score: float
    ) -> Tuple[float, str]:
        """
        ì¢…í•© ì ìˆ˜ ê³„ì‚°
        
        ê³„ì‚°ì‹:
        ì¢…í•©ì ìˆ˜ = (ì—°ì‚°ì ìˆ˜ Ã— 0.4) + (ë©”ëª¨ë¦¬ì ìˆ˜ Ã— 0.3) + (ì²˜ë¦¬ëŸ‰ì ìˆ˜ Ã— 0.3)
        
        ê°€ì¤‘ì¹˜ ê·¼ê±°:
        - ì—°ì‚° ì„±ëŠ¥ 40%: ë”¥ëŸ¬ë‹ í•™ìŠµì—ì„œ ê°€ì¥ ì¤‘ìš”
        - ë©”ëª¨ë¦¬ ëŒ€ì—­í­ 30%: ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµì‹œ ë³‘ëª©
        - ì´ë¯¸ì§€ ì²˜ë¦¬ëŸ‰ 30%: ì‹¤ì œ ì›Œí¬ë¡œë“œ ì„±ëŠ¥
        """
        print(f"\n[5/5] ìµœì¢… ì¢…í•© ì ìˆ˜ ê³„ì‚° ì¤‘...")
        
        overall = (
            compute_score * 0.4 +
            memory_score * 0.3 +
            throughput_score * 0.3
        )
        
        print(f"  â†’ ìˆ˜ì‹: ({compute_score:.1f} Ã— 0.4) + ({memory_score:.1f} Ã— 0.3) + ({throughput_score:.1f} Ã— 0.3)")
        print(f"  â†’ ê³„ì‚°: {compute_score*0.4:.1f} + {memory_score*0.3:.1f} + {throughput_score*0.3:.1f} = {overall:.1f}")
        
        # ë“±ê¸‰ ì‚°ì •
        if overall >= 90:
            rating = "S+ (ìµœìƒê¸‰)"
        elif overall >= 80:
            rating = "S (ìƒê¸‰)"
        elif overall >= 70:
            rating = "A+ (ìš°ìˆ˜)"
        elif overall >= 60:
            rating = "A (ì–‘í˜¸)"
        elif overall >= 50:
            rating = "B (ë³´í†µ)"
        else:
            rating = "C (ë¯¸í¡)"
        
        return overall, rating
    
    def run_full_benchmark(self) -> GPUBenchmarkResult:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("=" * 80)
        print(f"GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {self.device_name}")
        print("=" * 80)
        
        # 1. ì—°ì‚° ì„±ëŠ¥ ì¸¡ì •
        compute_tflops = self.measure_compute_performance()
        
        # 2. ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì¸¡ì •
        memory_bandwidth = self.measure_memory_bandwidth()
        
        # 3. FP32 ì²˜ë¦¬ëŸ‰ ì¸¡ì •
        fp32_throughput = self.measure_image_generation_throughput(
            batch_size=32, image_size=512, use_fp16=False
        )
        
        # 4. FP16 ì²˜ë¦¬ëŸ‰ ì¸¡ì •
        fp16_throughput = self.measure_image_generation_throughput(
            batch_size=32, image_size=512, use_fp16=True
        )
        
        # 5. ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°
        compute_score, memory_score, throughput_score = self.calculate_normalized_scores(
            compute_tflops, memory_bandwidth, fp16_throughput
        )
        
        # 6. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score, rating = self.calculate_overall_score(
            compute_score, memory_score, throughput_score
        )
        
        # 7. ë©”ëª¨ë¦¬ ì •ë³´
        memory_total = torch.cuda.get_device_properties(self.device_id).total_memory / 1024**3
        
        return GPUBenchmarkResult(
            compute_tflops=compute_tflops,
            memory_bandwidth_gbs=memory_bandwidth,
            fp16_throughput=fp16_throughput,
            fp32_throughput=fp32_throughput,
            memory_total_gb=memory_total,
            compute_score=compute_score,
            memory_score=memory_score,
            throughput_score=throughput_score,
            overall_score=overall_score,
            performance_rating=rating
        )

def print_benchmark_report(result: GPUBenchmarkResult, device_name: str):
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ğŸ“Š GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìµœì¢… ë³´ê³ ì„œ")
    print("=" * 80)
    
    print(f"\nğŸ–¥ï¸  GPU ëª¨ë¸: {device_name}")
    print(f"ğŸ’¾ ì´ ë©”ëª¨ë¦¬: {result.memory_total_gb:.2f} GB")
    
    print(f"\n{'â”€' * 80}")
    print("ğŸ“ˆ ì›ì‹œ ì„±ëŠ¥ ì¸¡ì •ê°’")
    print(f"{'â”€' * 80}")
    print(f"  âš¡ ì—°ì‚° ì„±ëŠ¥ (FP16):        {result.compute_tflops:>8.2f} TFLOPS")
    print(f"  ğŸš€ ë©”ëª¨ë¦¬ ëŒ€ì—­í­:           {result.memory_bandwidth_gbs:>8.2f} GB/s")
    print(f"  ğŸ–¼ï¸  FP32 ì´ë¯¸ì§€ ì²˜ë¦¬ëŸ‰:     {result.fp32_throughput:>8.2f} images/sec")
    print(f"  ğŸ–¼ï¸  FP16 ì´ë¯¸ì§€ ì²˜ë¦¬ëŸ‰:     {result.fp16_throughput:>8.2f} images/sec")
    print(f"  â±ï¸  FP16 ê°€ì† ë¹„ìœ¨:         {result.fp16_throughput/result.fp32_throughput:>8.2f}x")
    
    print(f"\n{'â”€' * 80}")
    print("ğŸ¯ ì •ê·œí™” ì ìˆ˜ (H100 ì´ë¡  ì„±ëŠ¥ ê¸°ì¤€)")
    print(f"{'â”€' * 80}")
    
    # ì ìˆ˜ ë°” ê·¸ë˜í”„
    def score_bar(score: float, width: int = 40) -> str:
        filled = int(score / 100 * width)
        return f"[{'â–ˆ' * filled}{'â–‘' * (width - filled)}]"
    
    print(f"  ì—°ì‚° ì„±ëŠ¥:   {score_bar(result.compute_score)} {result.compute_score:>5.1f}/100")
    print(f"  ë©”ëª¨ë¦¬ ì„±ëŠ¥: {score_bar(result.memory_score)} {result.memory_score:>5.1f}/100")
    print(f"  ì²˜ë¦¬ëŸ‰ ì„±ëŠ¥: {score_bar(result.throughput_score)} {result.throughput_score:>5.1f}/100")
    
    print(f"\n{'=' * 80}")
    print(f"ğŸ† ì¢…í•© ì„±ëŠ¥ ì ìˆ˜: {result.overall_score:.1f}/100 - {result.performance_rating}")
    print(f"{'=' * 80}")
    
    # ì„±ëŠ¥ í•´ì„
    print(f"\nğŸ’¡ ì„±ëŠ¥ í•´ì„:")
    if "H100" in device_name:
        print(f"  â€¢ ì´ë¡  ëŒ€ë¹„ ì‹¤ì œ ë‹¬ì„±ë¥ : ~{result.overall_score:.0f}%")
        print(f"  â€¢ ì—°ì‚° íš¨ìœ¨ì„±: {result.compute_score:.1f}% (ì´ë¡  989 TFLOPS ëŒ€ë¹„)")
        print(f"  â€¢ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {result.memory_score:.1f}% (ì´ë¡  3,350 GB/s ëŒ€ë¹„)")
    elif "A100" in device_name:
        h100_equiv = result.overall_score * (GPUPerformanceBenchmark.A100_REFERENCE['compute_tflops'] / 
                                            GPUPerformanceBenchmark.H100_REFERENCE['compute_tflops'])
        print(f"  â€¢ H100 í™˜ì‚° ì„±ëŠ¥: ~{h100_equiv:.0f}% ìˆ˜ì¤€")
        print(f"  â€¢ A100 ëŒ€ë¹„ H100ì€ ì´ë¡ ìƒ ì•½ 3.2ë°° ë¹ ë¦„")
    
    print(f"\nğŸ“ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ê¶Œì¥ ì‚¬í•­:")
    if result.fp16_throughput > 600:
        print(f"  â€¢ ë°°ì¹˜ í¬ê¸°: 32-64 (512x512 ê¸°ì¤€)")
        print(f"  â€¢ ì˜ˆìƒ í•™ìŠµ ì†ë„: ì‹œê°„ë‹¹ ì•½ {result.fp16_throughput * 3600 / 1000:.0f}K ì´ë¯¸ì§€")
    elif result.fp16_throughput > 400:
        print(f"  â€¢ ë°°ì¹˜ í¬ê¸°: 16-32 (512x512 ê¸°ì¤€)")
        print(f"  â€¢ ì˜ˆìƒ í•™ìŠµ ì†ë„: ì‹œê°„ë‹¹ ì•½ {result.fp16_throughput * 3600 / 1000:.0f}K ì´ë¯¸ì§€")
    else:
        print(f"  â€¢ ë°°ì¹˜ í¬ê¸°: 8-16 (512x512 ê¸°ì¤€)")
        print(f"  â€¢ ì˜ˆìƒ í•™ìŠµ ì†ë„: ì‹œê°„ë‹¹ ì•½ {result.fp16_throughput * 3600 / 1000:.0f}K ì´ë¯¸ì§€")
    
    print(f"  â€¢ FP16 Mixed Precision í•„ìˆ˜ ê¶Œì¥ ({result.fp16_throughput/result.fp32_throughput:.1f}x ê°€ì†)")
    print(f"  â€¢ Gradient Checkpointing ê³ ë ¤ (ë” í° ë°°ì¹˜ ê°€ëŠ¥)")

# ì‹¤í–‰
benchmark = GPUPerformanceBenchmark(device_id=0)
result = benchmark.run_full_benchmark()
print_benchmark_report(result, benchmark.device_name)

# JSON í˜•íƒœë¡œë„ ì €ì¥
result_dict = {
    'device': benchmark.device_name,
    'metrics': {
        'compute_tflops': result.compute_tflops,
        'memory_bandwidth_gbs': result.memory_bandwidth_gbs,
        'fp32_throughput': result.fp32_throughput,
        'fp16_throughput': result.fp16_throughput,
        'memory_total_gb': result.memory_total_gb,
    },
    'scores': {
        'compute': result.compute_score,
        'memory': result.memory_score,
        'throughput': result.throughput_score,
        'overall': result.overall_score,
        'rating': result.performance_rating,
    }
}

print(f"\nğŸ’¾ JSON ê²°ê³¼:")
print(json.dumps(result_dict, indent=2, ensure_ascii=False))


